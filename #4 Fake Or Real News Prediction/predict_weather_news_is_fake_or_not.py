# -*- coding: utf-8 -*-
"""Predict weather news is fake or not.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18NlH6R24233kwvKOZg-Z_ZJnuinZhyWz

About the dataset
1.   id: unique id for a news article.
2.   title: title of news article.
3.   author: author of news article.
4.   text: text of article, could be incomplete.
5.   label: lable that marks whether news article is real or fake.

```
1 -> Fake news 
0 -> Real news
```

Importing the Dependencies
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords # natural language tool kit - these are the text which don't add more value to our text
from nltk.stem.porter import PorterStemmer # remove the prefix & suffix return root
from sklearn.feature_extraction.text import TfidfVectorizer # text -> vector -> feature
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

print(stopwords.words('english'))
# These are the words that don't add much value to our new
# printing stopswords in english

"""Data Preprocessing"""

# loading the dataset to pandas DataFrame
n_d = pd.read_csv('/content/train.csv')

n_d.shape

# print top 5 row of dataframe
n_d.head()

# counting the missing value in the dataset
n_d.isnull().sum()

# replacing null value with empty string
n_d = n_d.fillna('')

# merging the title and author column 
n_d['content'] = n_d['author'] + ' ' + n_d['title']

print(n_d['content'])

# seprating data & label
x = n_d.drop(['label'],axis=1)
y = n_d['label']

print(x)
print(y)

"""Stemming
it is a process of reducing a word to its root word

ex:
acting, actor, actress -> act
"""

port_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]',' ',content)
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)
  return stemmed_content

# n_d['content'] = stemming(n_d['content']) my error
n_d['content'] = n_d['content'].apply(stemming)
print(n_d['content'])

# seprating data and label
x = n_d['content'].values # values apply to a column numpy
y = n_d['label'].values

print(x,y)

y.shape

# converting textual data into numberical data
vectorizer = TfidfVectorizer()
vectorizer.fit(x)
x = vectorizer.transform(x)

print(x)

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)

# stratify distribute in equal proportion of fake and real news
print(x.shape,x_train.shape,x_test.shape)

"""Training the model : Logistic Regression"""

model = LogisticRegression()

model.fit(x_train,y_train)

"""Evaluation"""

# accuracy on training data
x_train_predict = model.predict(x_train)
train_data_accuracy = accuracy_score(x_train_predict,y_train)

print('accuracy in training data : ',train_data_accuracy)

# accuracy on testing data
x_test_predict = model.predict(x_test)
test_data_accuracy = accuracy_score(x_test_predict,y_test)

print('accuracy in training data : ',test_data_accuracy)

"""Makeing predicition system"""

x_new = x_test[0]
predict = model.predict(x_new)

if(predict):
  print("Fake News üëè")
else:
  print("Real News üì∞")

print(y_test[0])

